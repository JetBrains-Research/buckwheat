{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary **modules**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import artm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tree_sitter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from tree_sitter import Language, Parser\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to get the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(number: int, delta: int) -> List:\n",
    "    \"\"\"\n",
    "    Creates a list of a given number of the datetime objects with a given step.\n",
    "    :param number: the amount of dates.\n",
    "    :param delta: the time step between dates\n",
    "    :return: a list of datetime objects.\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    date = datetime.now()\n",
    "    for i in range(number):\n",
    "        dates.append(date)\n",
    "        date = date - timedelta(days=delta)\n",
    "    dates.sort()\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2019, 10, 26, 0, 9, 43, 913184), datetime.datetime(2019, 11, 26, 0, 9, 43, 913184), datetime.datetime(2019, 12, 27, 0, 9, 43, 913184), datetime.datetime(2020, 1, 27, 0, 9, 43, 913184), datetime.datetime(2020, 2, 27, 0, 9, 43, 913184)]\n"
     ]
    }
   ],
   "source": [
    "dates = get_dates(5, 31)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to checkout a repository by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkout_by_date(repository: str, directory: str, date: datetime) -> None:\n",
    "    \"\"\"\n",
    "    Checkout a given repository into a folder for a given date and time.\n",
    "    :param repository: address of processed project.\n",
    "    :param directory: address of target directory for a checkout.\n",
    "    :param date: date and time of the last commit for the checkout\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    os.system('cp -r ' + repository + ' ' + directory)\n",
    "    os.system('(cd ' + directory + '; git checkout `git rev-list -n 1 --before=\"' + date.strftime('%Y-%m-%d') + '\" master`)')\n",
    "    # TODO: consider non-master branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkout_by_date('/home/diannao/java-design-patterns/', '/home/diannao/java-design-patterns_new', dates[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to get the file extensions for various languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extensions(lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the extension for a given language. TODO: more than one extension.\n",
    "    :param lang: language name.\n",
    "    :return: the extension.\n",
    "    \"\"\"\n",
    "    extensions = {'cpp': 'cpp',\n",
    "                  'java': 'java',\n",
    "                  'python': 'py'}\n",
    "    return extensions[lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to get a list of files with a given extension from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_list_of_files(directory: str, extension: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get a list of files with a given extension.\n",
    "    :param directory: the root directory that is studied.\n",
    "    :param extension: extension of the listed files.\n",
    "    :return: list of file paths.\n",
    "    \"\"\"\n",
    "    list_of_files = [y for x in os.walk(directory) for y in glob(os.path.join(x[0], '*.' + extension))]\n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_a_list_of_files('/home/diannao/PycharmProjects/topic-dynamics/topic_dynamics/tests/test_files/', 'java')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to read the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Read the contents of the file.\n",
    "    :param file: address of the file.\n",
    "    :return: bytes with the contents of the file.\n",
    "    \"\"\"\n",
    "    with open(file, 'r') as fin:\n",
    "        code = bytes(fin.read(), 'utf-8')\n",
    "    return code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to get the positional bytes of the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_bytes(node: tree_sitter.Node) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Extract start and end byte.\n",
    "    :param node: node on the AST.\n",
    "    :return: (start byte, end byte)\n",
    "    \"\"\"\n",
    "    start = node.start_byte\n",
    "    end = node.end_byte\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the utility functions for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSERS = {}\n",
    "\n",
    "\n",
    "def get_tree_sitter_dir() -> str:\n",
    "    \"\"\"\n",
    "    Get tree-sitter directory.\n",
    "    :return: absolute path.\n",
    "    \"\"\"\n",
    "    return '/home/diannao/PycharmProjects/topic-dynamics/topic_dynamics/parsers/'\n",
    "\n",
    "\n",
    "def get_tree_sitter_so() -> str:\n",
    "    \"\"\"\n",
    "    Get build tree-sitter `.so` location.\n",
    "    :return: absolute path.\n",
    "    \"\"\"\n",
    "    tree_sitter_dir = get_tree_sitter_dir()\n",
    "    bin_loc = os.path.join(tree_sitter_dir, \"build/langs.so\")\n",
    "    return bin_loc\n",
    "\n",
    "\n",
    "def main_parse() -> None:\n",
    "    \"\"\"\n",
    "    Initialize tree-sitter library.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # root directory for tree-sitter\n",
    "    tree_sitter_dir = get_tree_sitter_dir()\n",
    "    # grammar locations\n",
    "    c_grammar_loc = os.path.join(tree_sitter_dir, \"vendor/tree-sitter-c\")\n",
    "    c_sharp_grammar_loc = os.path.join(tree_sitter_dir, \"vendor/tree-sitter-c-sharp\")\n",
    "    cpp_grammar_loc = os.path.join(tree_sitter_dir, \"vendor/tree-sitter-cpp\")\n",
    "    java_grammar_loc = os.path.join(tree_sitter_dir, \"vendor/tree-sitter-java\")\n",
    "    python_grammar_loc = os.path.join(tree_sitter_dir, \"vendor/tree-sitter-python\")\n",
    "    # location for library\n",
    "    bin_loc = get_tree_sitter_so()\n",
    "    # build everything\n",
    "    Language.build_library(\n",
    "        # Store the library in the `bin_loc`\n",
    "        bin_loc,\n",
    "        # Include languages\n",
    "        [\n",
    "            c_grammar_loc,\n",
    "            c_sharp_grammar_loc,\n",
    "            cpp_grammar_loc,\n",
    "            java_grammar_loc,\n",
    "            python_grammar_loc\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_parser(lang: str) -> Parser:\n",
    "    \"\"\"\n",
    "    Initialize parser for a specific language.\n",
    "    :param lang: language to use.\n",
    "    :return: parser.\n",
    "    \"\"\"\n",
    "    global PARSERS\n",
    "    if lang not in PARSERS:\n",
    "        parser = Parser()\n",
    "        parser.set_language(Language(get_tree_sitter_so(), lang))\n",
    "        PARSERS[lang] = parser\n",
    "    else:\n",
    "        parser = PARSERS[lang]\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to get the identifiers of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifiers(file: str, lang: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Gather a sorted list of identifiers in the file and their count.\n",
    "    :param file: address of the file.\n",
    "    :param lang: the language of file.\n",
    "    :return: a list of tuples, identifier and count.\n",
    "    \"\"\"\n",
    "    code = read_file(file)\n",
    "    tree = get_parser(lang).parse(code)\n",
    "    root = tree.root_node\n",
    "    identifiers = []\n",
    "    node_types = {'c': ['identifier', 'type_identifier'],\n",
    "                  'c-sharp': ['identifier', 'type_identifier'],\n",
    "                  'cpp': ['identifier', 'type_identifier'],\n",
    "                  'java': ['identifier', 'type_identifier'],\n",
    "                  'python': ['identifier', 'type_identifier']}\n",
    "\n",
    "    def traverse_tree(node: tree_sitter.Node) -> None:\n",
    "        \"\"\"\n",
    "        Run down the AST from a given node and gather identifiers from its childern.\n",
    "        :param node: starting node.\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "        for child in node.children:\n",
    "            if child.type in node_types[lang]:\n",
    "                start, end = get_positional_bytes(child)\n",
    "                identifier = code[start:end].decode('utf-8').lower()\n",
    "                if '\\n' not in identifier:  # Will break output files. Can add other bad characters later\n",
    "                    identifiers.append(identifier)\n",
    "            if len(child.children) != 0:\n",
    "                traverse_tree(child)\n",
    "\n",
    "    traverse_tree(root)\n",
    "    sorted_identifiers = sorted(Counter(identifiers).items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    return sorted_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 9), ('anarray', 6), ('length', 2), ('system', 2), ('out', 2), ('arraydemo', 1), ('main', 1), ('string', 1), ('args', 1), ('print', 1), ('println', 1)]\n"
     ]
    }
   ],
   "source": [
    "identifiers = get_identifiers('../../tests/test_files/test.java', 'java')\n",
    "print(identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to transform the identifiers into a writeable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_identifiers(identifiers: List) -> List[str]:\n",
    "    \"\"\"\n",
    "    Transform the original list of identifiers into the writable form.\n",
    "    :param identifiers: list of tuples, identifier and count.\n",
    "    :return: a list of identifiers in the writable for, \"identifier:count\".\n",
    "    \"\"\"\n",
    "    formatted_identifiers = []\n",
    "    for identifier in identifiers:\n",
    "        if identifier[0].rstrip() != '':  # Checking for occurring empty tokens.\n",
    "            formatted_identifiers.append(identifier[0].rstrip() + ':' + str(identifier[1]).rstrip())\n",
    "    return formatted_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i:9', 'anarray:6', 'length:2', 'system:2', 'out:2', 'arraydemo:1', 'main:1', 'string:1', 'args:1', 'print:1', 'println:1']\n"
     ]
    }
   ],
   "source": [
    "formatted = transform_identifiers(identifiers)\n",
    "print (formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the functions that transform the tokens into UCI bag of words format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uci_format(directory: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Transform the temporary file with tokens into the UCI bag-of-words format.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    number_of_documents = 0\n",
    "    number_of_nnz = 0\n",
    "    set_of_tokens = set()\n",
    "    with open(os.path.abspath(os.path.join(directory, name + '_tokens.txt')), 'r') as fin:\n",
    "        for line in fin:\n",
    "            number_of_documents = number_of_documents + 1\n",
    "            for token in line.rstrip().split(';')[2].split(','):\n",
    "                number_of_nnz = number_of_nnz + 1\n",
    "                set_of_tokens.add(token.split(':')[0])\n",
    "    number_of_tokens = len(set_of_tokens)\n",
    "    sorted_list_of_tokens = sorted(list(set_of_tokens))\n",
    "    sorted_dictionary_of_tokens = {}\n",
    "    with open(os.path.abspath(os.path.join(directory, 'vocab.' + name + '.txt')), 'w+') as fout:\n",
    "        for index in range(len(sorted_list_of_tokens)):\n",
    "            sorted_dictionary_of_tokens[sorted_list_of_tokens[index]] = index + 1\n",
    "            fout.write(sorted_list_of_tokens[index] + '\\n')\n",
    "    with open(os.path.abspath(os.path.join(directory, name + '_tokens.txt')), 'r') as fin, open(os.path.abspath(os.path.join(directory, 'docword.' + name + '.txt')), 'w+') as fout:\n",
    "        fout.write(str(number_of_documents) + '\\n' + str(number_of_tokens) + '\\n' + str(number_of_nnz) + '\\n')\n",
    "        for line in fin:\n",
    "            file_tokens = line.rstrip().split(';')[2].split(',')\n",
    "            file_tokens_separated = []\n",
    "            file_tokens_separated_numbered = []\n",
    "            for entry in file_tokens:\n",
    "                file_tokens_separated.append(entry.split(':'))\n",
    "            for entry in file_tokens_separated:\n",
    "                file_tokens_separated_numbered.append([sorted_dictionary_of_tokens[entry[0]], int(entry[1])])\n",
    "            file_tokens_separated_numbered = sorted(file_tokens_separated_numbered, key=itemgetter(0), reverse=False)\n",
    "            for entry in file_tokens_separated_numbered:\n",
    "                fout.write(str(line.split(';')[0]) + ' ' + str(entry[0]) + ' ' + str(entry[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the main function that slices and tokenizes the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_the_repository(repository: str, number: int, delta: int, lang: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Split the repository, parse the files, write the data into a file.\n",
    "    :param repository: path to the repository to process.\n",
    "    :param number: the amount of dates.\n",
    "    :param delta: the time step between dates\n",
    "    :param lang: language of parsing.\n",
    "    :param name: name of the dataset (directories with resulting files)\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    directory = os.path.abspath(os.path.join(repository, os.pardir, name + '_processed'))\n",
    "    os.mkdir(directory)\n",
    "    dates = get_dates(number, delta)\n",
    "    lists_of_files = {}\n",
    "    for date in dates:\n",
    "        subdirectory = os.path.abspath(os.path.join(directory, date.strftime('%Y-%m-%d')))\n",
    "        checkout_by_date(repository, subdirectory, date)\n",
    "        lists_of_files[date.strftime('%Y-%m-%d')] = get_a_list_of_files(subdirectory, get_extensions(lang))\n",
    "    indexes_of_slices = {}\n",
    "    count = 0\n",
    "    with open(os.path.abspath(os.path.join(directory, name + '_tokens.txt')), 'w+') as fout:\n",
    "        for date in dates:\n",
    "            starting_index = count + 1\n",
    "            for file in lists_of_files[date.strftime('%Y-%m-%d')]:\n",
    "                if os.path.isfile(file):  # TODO: implement a better file-checking mechanism\n",
    "                    try:\n",
    "                        identifiers = get_identifiers(file, lang)\n",
    "                        if len(identifiers) != 0:\n",
    "                            count += 1\n",
    "                            formatted_identifiers = transform_identifiers(identifiers)\n",
    "                            fout.write(str(count) + ';' + file + ';' + ','.join(formatted_identifiers) + '\\n')\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            ending_index = count\n",
    "            indexes_of_slices[date.strftime('%Y-%m-%d')] = (starting_index, ending_index)\n",
    "    with open(os.path.abspath(os.path.join(directory, name + '_tokens_info.txt')), 'w+') as fout:\n",
    "        for date in indexes_of_slices.keys():\n",
    "            fout.write(date + ';' + str(indexes_of_slices[date][0]) + ',' + str(indexes_of_slices[date][1]) + '\\n')\n",
    "    uci_format(directory, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_the_repository('/home/diannao/Documents/java-design-patterns/', 36, 31, 'java', \n",
    "     'java-design-patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create **batches** and the **dictionary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(directory: str, name: str) -> Tuple[artm.BatchVectorizer, artm.Dictionary]:\n",
    "    \"\"\"\n",
    "    Create the batches and the dictionary from the dataset.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :return: BatchVectorizer and Dictionary.\n",
    "    \"\"\"\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=directory, data_format='bow_uci',\n",
    "                                        collection_name=name, target_folder=os.path.abspath(os.path.join(directory, name + '_batches')))\n",
    "    dictionary = batch_vectorizer.dictionary\n",
    "    return batch_vectorizer, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, dictionary = create_batches('/home/diannao/Documents/java-design-patterns_processed/', 'java-design-patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(number_of_topics: int, dictionary: artm.Dictionary, SparceTheta: float, SparsePhi: float, DecorrelatorPhi: float) -> artm.artm_model.ARTM:   \n",
    "    \"\"\"\n",
    "    Define ARTM model.\n",
    "    :param number_of_topics: number of topics.\n",
    "    :param dictionary: Batch Vectorizer dictionary.\n",
    "    :param SparceTheta: Sparse Theta Parameter.\n",
    "    :param SparcePhi: Sparse Phi Parameter.\n",
    "    :param DecorrelatorPhi: Decorellator Phi Parameter.\n",
    "    :return: ARTM model.\n",
    "    \"\"\"\n",
    "    topic_names = ['topic_{}'.format(i) for i in range(1, number_of_topics + 1)]\n",
    "    model_artm = artm.ARTM(topic_names=topic_names, cache_theta=True,\n",
    "                             scores=[artm.PerplexityScore(name='PerplexityScore',\n",
    "                                                          dictionary=dictionary),\n",
    "                                     artm.SparsityPhiScore(name='SparsityPhiScore'),\n",
    "                                     artm.SparsityThetaScore(name='SparsityThetaScore'),\n",
    "                                     artm.TopicKernelScore(name='TopicKernelScore',\n",
    "                                                           probability_mass_threshold=0.3),\n",
    "                                     artm.TopTokensScore(name='TopTokensScore', num_tokens=15)],\n",
    "                             regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta',\n",
    "                                                                             tau=SparceTheta),\n",
    "                                           artm.SmoothSparsePhiRegularizer(name='SparsePhi', tau=SparsePhi),\n",
    "                                           artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=DecorrelatorPhi)])\n",
    "    return model_artm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artm = define_model(50,dictionary,-0.15,-0.1,1.5e+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: artm.artm_model.ARTM, number_of_document_passes: int, number_of_collection_passes: int, \n",
    "                dictionary: artm.Dictionary, batch_vectorizer: artm.BatchVectorizer) -> None:\n",
    "    \"\"\"\n",
    "    Train the ARTM model.\n",
    "    :param model: the trained model.\n",
    "    :param number_of_document_passes: number of document passes.\n",
    "    :param number_of_collection_passes: number of collection passes.\n",
    "    :param dictionary: Batch Vectorizer dictionary.\n",
    "    :param batch_vectorizer: Batch Vectorizer.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    model.num_document_passes = number_of_document_passes\n",
    "    model.initialize(dictionary=dictionary)\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=number_of_collection_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_artm, 1, 25, dictionary, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(model: artm.artm_model.ARTM, directory: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the parameters of the model.\n",
    "    :param model: the model.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    with open(os.path.abspath(os.path.join(directory, 'results', name + '_parameters.txt')), 'w+') as fout:\n",
    "        fout.write('Sparsity Phi: {0:.3f}'.format(\n",
    "            model.score_tracker['SparsityPhiScore'].last_value) + '\\n')\n",
    "        fout.write('Sparsity Theta: {0:.3f}'.format(\n",
    "            model.score_tracker['SparsityThetaScore'].last_value) + '\\n')\n",
    "        fout.write('Kernel contrast: {0:.3f}'.format(\n",
    "            model.score_tracker['TopicKernelScore'].last_average_contrast) + '\\n')\n",
    "        fout.write('Kernel purity: {0:.3f}'.format(\n",
    "            model.score_tracker['TopicKernelScore'].last_average_purity) + '\\n')\n",
    "        fout.write('Perplexity: {0:.3f}'.format(\n",
    "            model.score_tracker['PerplexityScore'].last_value) + '\\n')\n",
    "        \n",
    "    plt.plot(range(model.num_phi_updates),\n",
    "             model.score_tracker['PerplexityScore'].value, 'r--', linewidth=2)\n",
    "    plt.xlabel('Iterations count')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.abspath(os.path.join(directory, 'results', name + '_perplexity.png')), dpi = 300)\n",
    "    plt.close()\n",
    "    \n",
    "    plt.plot(range(model.num_phi_updates),\n",
    "             model.score_tracker['SparsityPhiScore'].value, 'r--', linewidth=2)\n",
    "    plt.xlabel('Iterations count')\n",
    "    plt.ylabel('Phi Sparsity')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.abspath(os.path.join(directory, 'results', name + '_phi_sparsity.png')), dpi = 300)\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(range(model.num_phi_updates),\n",
    "             model.score_tracker['SparsityThetaScore'].value, 'r--', linewidth=2)\n",
    "    plt.xlabel('Iterations count')\n",
    "    plt.ylabel('Theta Sparsity')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.abspath(os.path.join(directory, 'results', name + '_theta_sparsity.png')), dpi = 300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the most popular tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_most_popular_tokens(model: artm.artm_model.ARTM, directory: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the most popular tokens of the model.\n",
    "    :param model: the model.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    with open(os.path.abspath(os.path.join(directory, 'results', name + '_most_popular_tokens.txt')), 'w+') as fout:\n",
    "        for topic_name in model.topic_names:\n",
    "            fout.write(topic_name + ' : ' + str(model.score_tracker['TopTokensScore'].last_tokens[topic_name]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matrices(model: artm.artm_model.ARTM, directory: str, name: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Save the Phi and Theta matrices.\n",
    "    :param model: the model.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :return: Two matrices as DataFrames.\n",
    "    \"\"\"\n",
    "    phi_matrix = model.get_phi().sort_index(axis=0)\n",
    "    phi_matrix.to_csv(os.path.abspath(os.path.join(directory, 'results', name + '_phi.csv')))\n",
    "    theta_matrix = model.get_theta().sort_index(axis=1)\n",
    "    theta_matrix.to_csv(os.path.abspath(os.path.join(directory, 'results', name + '_theta.csv')))\n",
    "    return phi_matrix, theta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the most topical files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_most_topical_files(number_of_topics: int, theta: pd.DataFrame, directory: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the most topical files of the model.\n",
    "    :param number_of_topics: the number of topics.\n",
    "    :param theta: Theta matrix.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    file_address = {}\n",
    "    with open(os.path.abspath(os.path.join(directory, name + '_tokens.txt')), 'r') as fin:\n",
    "        for line in fin:\n",
    "            file_address[int(line.split(';')[0])] = line.split(';')[1]\n",
    "    with open(os.path.abspath(os.path.join(directory, 'results', name + '_most_topical_files.txt')), 'w+') as fout:\n",
    "        for i in range(1, number_of_topics + 1):\n",
    "            fout.write('Topic ' + str(i) + '\\n\\n')\n",
    "            dictionary_of_the_topic = theta.sort_values(by='topic_' + str(i), axis=1, ascending=False).loc['topic_' + str(i)][:10].to_dict()\n",
    "            for j in dictionary_of_the_topic.keys():\n",
    "                fout.write(str(j) + ' ' + str(dictionary_of_the_topic[j]) + ' ' + file_address[int(j)] + '\\n')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dynamics(directory: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save figures with the dynamics.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    indexes = {}\n",
    "    with open(os.path.abspath(os.path.join(directory, name + '_tokens_info.txt')), 'r') as fin:\n",
    "        for line in fin:\n",
    "            indexes[line.rstrip().split(';')[0]] = (int(line.rstrip().split(';')[1].split(',')[0]),\n",
    "                                                    int(line.rstrip().split(';')[1].split(',')[1]))\n",
    "    topics_weight = []\n",
    "    with open(os.path.abspath(os.path.join(directory, 'results', name + '_theta.csv')), 'r') as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            topics_weight.append([])\n",
    "            for year in indexes.keys():\n",
    "                topics_weight[-1].append(sum(float(i) for i in row[indexes[year][0]:indexes[year][1] + 1]))\n",
    "    topics_weight = np.asarray(topics_weight)\n",
    "    topics_weight_percent = np.zeros((topics_weight.shape[0], topics_weight.shape[1]))\n",
    "    for i in range(topics_weight.shape[0]):\n",
    "        for j in range(topics_weight.shape[1]):\n",
    "            topics_weight_percent[i, j] = topics_weight[i, j] / np.sum(topics_weight[:, j], keepdims=True) * 100\n",
    "    np.savetxt(os.path.abspath(os.path.join(directory, 'results', name + '_dynamics.txt')), topics_weight, '%10.5f')\n",
    "    np.savetxt(os.path.abspath(os.path.join(directory, 'results', name + '_dynamics_percent.txt')), topics_weight_percent, '%10.5f')\n",
    "    dynamics = []\n",
    "    for i in range(topics_weight_percent.shape[0]):\n",
    "        dynamics.append(['topic_{}'.format(i + 1), min(topics_weight_percent[i]),\n",
    "                         max(topics_weight_percent[i]), max(topics_weight_percent[i]) / min(topics_weight_percent[i])])\n",
    "    dynamics = sorted(dynamics, key = itemgetter(3), reverse=True)\n",
    "    with open(os.path.abspath(os.path.join(directory, 'results', name + '_dynamics_percent_change.txt')), 'w+') as fout:\n",
    "        for item in dynamics:\n",
    "            fout.write(item[0] + '\\t' + str(format(item[1], '.5f')) + '\\t' + str(format(item[2], '.5f')) + '\\t' + str(format(item[3], '.5f')) + '\\n')\n",
    "    \n",
    "    plt.stackplot(indexes.keys(), topics_weight)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Proportion (a. u.)')\n",
    "    plt.savefig(os.path.abspath(os.path.join(directory, 'results', name + '_dynamics.png')), dpi = 300)\n",
    "    plt.close()\n",
    "\n",
    "    plt.stackplot(indexes.keys(), topics_weight_percent)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Proportion (%)')\n",
    "    plt.savefig(os.path.abspath(os.path.join(directory, 'results', name + '_dynamics_percent.png')), dpi = 300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commence all the writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_data(model: artm.artm_model.ARTM, directory: str, name: str, number_of_topics: int) -> None:\n",
    "    \"\"\"\n",
    "    Save the parameters of the model.\n",
    "    :param model: the model.\n",
    "    :param directory: the directory with the dataset.\n",
    "    :param name: name of the processed dataset.\n",
    "    :param number_of_topics: the number of topics.\n",
    "    :param theta: Theta matrix.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.abspath(os.path.join(directory, 'results'))):\n",
    "        os.makedirs(os.path.abspath(os.path.join(directory, 'results')))\n",
    "    save_parameters(model, directory, name)\n",
    "    save_most_popular_tokens(model, directory, name)\n",
    "    phi_matrix, theta_matrix = save_matrices(model, directory, name)\n",
    "    save_most_topical_files(number_of_topics, theta_matrix, directory, name)\n",
    "    save_dynamics(directory, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_all_data(model_artm, '/home/diannao/Documents/java-design-patterns_processed/', 'java-design-patterns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
